<!DOCTYPE html>
<html><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Generative AI Learning Path - Modules Overview</title>
    <style>
        /* Basic Durham styling */
        body {
            font-family: Arial, sans-serif;
            background-color: #f5f5f5;
            color: #003A70;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
        }
        h1, h2, h3 {
            color: #00743E;
        }
        h1 {
            text-align: center;
            margin-bottom: 20px;
        }
        .content-section {
            margin-top: 20px;
        }
        .quiz-section {
            background-color: #E6F3EF;
            padding: 15px;
            border-left: 4px solid #00743E;
            border-radius: 4px;
            margin-top: 20px;
        }
        .quiz-section p {
            margin-bottom: 10px;
        }
        .quiz-section label {
            display: block;
            margin-bottom: 5px;
        }
        button {
            background-color: #00743E;
            color: #FFF;
            padding: 10px 20px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        .result {
            margin-top: 10px;
            font-weight: bold;
        }
        iframe {
            width: 100%;
            height: 315px;
            margin-top: 10px;
        }
        a {
            color: #003A70;
            text-decoration: underline;
        }
    </style>
</head><body><div class="container">
<h1>Generative AI Learning Path - Modules Overview</h1>
<!-- Module 1: Introduction to Large Language Models -->
<div class="content-section">
<h2>Module 1: Introduction to Large Language Models</h2>
<p>This module introduces Large Language Models (LLMs), covering concepts like pre-training and fine-tuning, prompt design, and tools like Google’s PaLM API and Vertex AI. Learn how LLMs are trained and how they can perform diverse language tasks with minimal field training data.</p>
<iframe src="https://www.youtube.com/embed/zizonToFXDs?si=b265KrF_vnsqyBzg" title="Introduction to Large Language Models"></iframe>
<p><a href="https://www.youtube.com/watch?v=zizonToFXDs?si=b265KrF_vnsqyBzg">View Transcript</a></p>
<!-- Quiz for Module 1 -->
<div class="quiz-section">
<h2>Quiz - Module 1</h2>
<p>1. What are large language models (LLMs) primarily used for?</p>
<label><input type="radio" name="q1_1" value="0"> Only machine translation</label> <label><input type="radio" name="q1_1" value="1"> Various language processing tasks like text classification, summarization, etc.</label>
<p>2. What is prompt engineering?</p>
<label><input type="radio" name="q1_2" value="1"> The process of designing prompts to improve LLM performance</label> <label><input type="radio" name="q1_2" value="0"> Training LLMs on large datasets</label> <button onclick="checkQuiz(1)">Submit Answers</button>
<p class="result" id="result1"></p>
</div>
</div>
<!-- Module 2: Attention Mechanism Overview -->
<div class="content-section">
<h2>Module 2: Attention Mechanism Overview</h2>
<p>This module explores the attention mechanism, a key concept for focusing on specific parts of an input sequence in tasks like translation and summarization. It explains how attention weights are assigned and used to enhance performance.</p>
<iframe src="https://www.youtube.com/embed/fjJOgb-E41w?si=4imuZhcpzKysoHdn" title="Attention Mechanism Overview"></iframe>
<p><a href="https://www.youtube.com/watch?v=fjJOgb-E41w?si=4imuZhcpzKysoHdn">View Transcript</a></p>
<!-- Quiz for Module 2 -->
<div class="quiz-section">
<h2>Quiz - Module 2</h2>
<p>1. What is the purpose of the attention mechanism in sequence-to-sequence models?</p>
<label><input type="radio" name="q2_1" value="0"> To ignore irrelevant input data</label> <label><input type="radio" name="q2_1" value="1"> To focus on relevant parts of the input sequence</label>
<p>2. What does the attention layer calculate for each part of the input?</p>
<label><input type="radio" name="q2_2" value="1"> Weights representing importance</label> <label><input type="radio" name="q2_2" value="0"> Exact positions in output</label> <button onclick="checkQuiz(2)">Submit Answers</button>
<p class="result" id="result2"></p>
</div>
</div>
<!-- Module 3: Transformer and BERT Model Overview -->
<div class="content-section">
<h2>Module 3: Transformer and BERT Model Overview</h2>
<p>This module dives into the Transformer model and BERT, discussing key aspects such as the self-attention mechanism, transformer layers, and applications of BERT in tasks like question answering and text classification.</p>
<iframe src="https://www.youtube.com/embed/t45S_MwAcOw?si=EJtFVgklPHU3vUtG" title="Transformer and BERT Model Overview"></iframe>
<p><a href="https://www.youtube.com/watch?v=t45S_MwAcOw?si=EJtFVgklPHU3vUtG">View Transcript</a></p>
<!-- Quiz for Module 3 -->
<div class="quiz-section">
<h2>Quiz - Module 3</h2>
<p>1. What is the purpose of the self-attention layer in Transformers?</p>
<label><input type="radio" name="q3_1" value="1"> To allow the model to focus on relevant words in context</label> <label><input type="radio" name="q3_1" value="0"> To reduce the model’s complexity</label>
<p>2. What does BERT stand for?</p>
<label><input type="radio" name="q3_2" value="1"> Bidirectional Encoder Representations from Transformers</label> <label><input type="radio" name="q3_2" value="0"> Binary Encoding and Recoding Transformers</label> <button onclick="checkQuiz(3)">Submit Answers</button>
<p class="result" id="result3"></p>
</div>
</div>
<!-- Module 4: Encoder-Decoder Architecture Overview -->
<div class="content-section">
<h2>Module 4: Encoder-Decoder Architecture Overview</h2>
<p>This module covers the encoder-decoder architecture, used in sequence-to-sequence tasks like machine translation. It explains the role of the encoder in generating a vector representation and the decoder in generating output sequences.</p>
<iframe src="https://www.youtube.com/embed/zbdong_h-x4?si=DmBRm0xIjiaynczz" title="Encoder-Decoder Architecture Overview"></iframe>
<p><a href="https://www.youtube.com/watch?v=zbdong_h-x4?si=DmBRm0xIjiaynczz">View Transcript</a></p>
<!-- Quiz for Module 4 -->
<div class="quiz-section">
<h2>Quiz - Module 4</h2>
<p>1. What is the main function of the encoder in the encoder-decoder model?</p>
<label><input type="radio" name="q4_1" value="1"> To create a vector representation of the input</label> <label><input type="radio" name="q4_1" value="0"> To directly generate the final output</label>
<p>2. What technique is commonly used during training to ensure accurate outputs?</p>
<label><input type="radio" name="q4_2" value="1"> Teacher forcing</label> <label><input type="radio" name="q4_2" value="0"> Model fine-tuning</label> <button onclick="checkQuiz(4)">Submit Answers</button>
<p class="result" id="result4"></p>
</div>
</div>
</div>
<!-- JavaScript for Quiz Functionality -->
<p><script>
        function checkQuiz(module) {
            const answers = {
                1: ["1", "1"],
                2: ["1", "1"],
                3: ["1", "1"],
                4: ["1", "1"]
            };
            let score = 0;
            const totalQuestions = answers[module].length;

            for (let i = 1; i <= totalQuestions; i++) {
                const selected = document.querySelector(`input[name="q${module}_${i}"]:checked`);
                if (selected && selected.value === answers[module][i - 1]) {
                    score++;
                }
            }

            const result = document.getElementById(`result${module}`);
            result.textContent = score === totalQuestions ? 
                "Correct! You got all answers right!" : 
                `You got ${score} out of ${totalQuestions} correct. Try again!`;
            result.style.color = score === totalQuestions ? "green" : "red";
        }
    </script></p></body></html>