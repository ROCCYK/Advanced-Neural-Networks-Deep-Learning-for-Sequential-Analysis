<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module Summary and Next Week Overview</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.8;
            font-size: 18px;
            color: #333;
        }
        h2, h3 {
            color: #007a33; /* Durham College Green */
        }
        h2 {
            font-size: 32px;
        }
        h3 {
            font-size: 28px;
        }
        p, ul, ol {
            margin-bottom: 20px;
        }
        ul, ol {
            font-size: 18px;
        }
        .summary-activity, .next-week {
            padding: 20px;
            border-radius: 8px;
        }
        .summary-activity {
            background-color: #f2f2f2; /* Light grey */
        }
        .next-week {
            background-color: #d8e8e2; /* Light Durham College Green */
        }
        .next-week ul, .summary-activity ol {
            margin-left: 20px;
        }
        footer {
            font-size: 16px;
            margin-top: 40px;
            color: #666;
        }
    </style>
</head><body><h2>Summary</h2>
<p>In this module, we explored the foundational concepts of machine learning in PyTorch, including:</p>
<ul>
<li><strong>The Perceptron</strong>: A single-layer neural network that forms the building block of more complex models.</li>
<li><strong>Basic Neural Networks (NNs)</strong>: We covered the architecture of simple neural networks and how they work.</li>
<li><strong>Hidden Layers</strong>: Layers between the input and output, enabling the network to model complex data patterns.</li>
<li><strong>Activation Functions</strong>: Functions such as ReLU, Sigmoid, and Tanh that help introduce non-linearity into the network.</li>
</ul>
<p>These concepts form the basis for more advanced neural network models and deep learning techniques.</p>
<h2>Next Week</h2>
<p>Next week, we will dive deeper into the mechanics of training neural networks by exploring:</p>
<ul>
<li><strong>Backpropagation</strong>: The algorithm used to calculate gradients and update weights in a neural network.</li>
<li><strong>Gradient Descent</strong>: Techniques like Stochastic Gradient Descent (SGD) and how they help optimize neural networks.</li>
</ul>
<footer>Â© Durham College. All rights reserved.</footer></body></html>